{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN6buZC4f/ciUEuGdK5rt0D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M-SAAD-BIN-MAZHAR/100DaysOFDeeplearning/blob/main/TransferlearningUsingAgumentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wsT-p2bjxUPQ"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d salader/dogs-vs-cats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjzT9CO6jnAG",
        "outputId": "8ca17c6c-dede-4571-874e-39be6bbfbff7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/salader/dogs-vs-cats\n",
            "License(s): unknown\n",
            "Downloading dogs-vs-cats.zip to /content\n",
            "100% 1.06G/1.06G [00:05<00:00, 174MB/s]\n",
            "100% 1.06G/1.06G [00:05<00:00, 213MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/dogs-vs-cats.zip', 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "FcVNMAtDj01B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,array_to_img,img_to_array,load_img\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n"
      ],
      "metadata": {
        "id": "dPG77clwj3Qo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base=VGG16(weights='imagenet',\n",
        "                include_top=False,\n",
        "                input_shape=(150,150,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2wFULtykU8Y",
        "outputId": "56c97643-338e-41a3-a935-9dab090bed51"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))"
      ],
      "metadata": {
        "id": "kdcRPOhxk862"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.trainable=False"
      ],
      "metadata": {
        "id": "XNq0hq76lXO9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "# this is the augmentation configuration we will use for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "# this is the augmentation configuration we will use for testing:\n",
        "# only rescaling\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# this is a generator that will read pictures found in\n",
        "# subfolers of 'data/train', and indefinitely generate\n",
        "# batches of augmented image data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/dogs_vs_cats/train',  # this is the target directory\n",
        "        target_size=(150, 150),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
        "\n",
        "# this is a similar generator, for validation data\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        '/content/dogs_vs_cats/test',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZk0Ou3El4oI",
        "outputId": "a3fdf5f4-1269-4991-e1ba-114379dfc71a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 images belonging to 2 classes.\n",
            "Found 5000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
      ],
      "metadata": {
        "id": "nLMpAk3pn8-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "gkyt_aRGmctc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01cJpOgkmr4N",
        "outputId": "e6fc74ea-7efa-4d42-8afa-6eacd4a05961"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 152ms/step - accuracy: 0.6569 - loss: 1.4333 - val_accuracy: 0.7788 - val_loss: 0.4965\n",
            "Epoch 2/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 141ms/step - accuracy: 0.8135 - loss: 0.4163 - val_accuracy: 0.8112 - val_loss: 0.3865\n",
            "Epoch 3/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 206ms/step - accuracy: 0.8617 - loss: 0.3469 - val_accuracy: 0.8250 - val_loss: 0.3840\n",
            "Epoch 4/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 136ms/step - accuracy: 0.8610 - loss: 0.3175 - val_accuracy: 0.8988 - val_loss: 0.2293\n",
            "Epoch 5/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 207ms/step - accuracy: 0.8614 - loss: 0.3073 - val_accuracy: 0.8188 - val_loss: 0.4193\n",
            "Epoch 6/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 111ms/step - accuracy: 0.8480 - loss: 0.3457 - val_accuracy: 0.8863 - val_loss: 0.2754\n",
            "Epoch 7/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 126ms/step - accuracy: 0.8813 - loss: 0.3010 - val_accuracy: 0.8925 - val_loss: 0.2562\n",
            "Epoch 8/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8836 - loss: 0.2840 - val_accuracy: 0.8725 - val_loss: 0.3134\n",
            "Epoch 9/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 206ms/step - accuracy: 0.8705 - loss: 0.2944 - val_accuracy: 0.8775 - val_loss: 0.2838\n",
            "Epoch 10/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 86ms/step - accuracy: 0.8827 - loss: 0.2737 - val_accuracy: 0.8950 - val_loss: 0.2368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6QPmgsbgm1j4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}